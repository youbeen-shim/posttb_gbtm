---
title: "GBTM Implementation via LCMM"
output: html_notebook
---

```{r}
library(tidyverse)
library(lcmm)
library(ggplot2)
library(gridExtra)
library(knitr)
library(corrplot)

patient_demographics <- read_csv("tb_patient_demographics.csv")
quarterly_visits <- read_csv("tb_quarterly_visits.csv")
```

```{r}
lcmm_data <- quarterly_visits %>%
  left_join(patient_demographics, by = "patient_id") %>%
  arrange(patient_id, quarter_id) %>%
  mutate(
    patient_numeric = as.numeric(factor(patient_id)), # forcing to numeric
    time = quarter_id - 1 # zero indexing
  ) %>%
  mutate( # converting (to) binary indicators
    male = ifelse(sex == "male", 1, 0),
    ds_tb = ifelse(drug_resistance == "Drug-sensitive", 1, 0), # not distinguishing MDR and XDR
    lung = ifelse(tb_type == "Pulmonary", 1, 0),
    sputum = ifelse(baseline_sputum_smear == "Positive", 1, 0)
  ) %>%
  # TODO: A more robust missing data processing ** 
  filter (!is.na(hospital_visits))

nrow(lcmm_data)
length(unique(quarterly_visits$patient_id)) # any missing?
length(unique(lcmm_data$patient_id))
length(unique(lcmm_data$time))
min(lcmm_data$time)
max(lcmm_data$time)
```

Spaghetti plot to observe the trajectories
```{r}
# reduce the number of patients observed to actually be able to see 
sample_patients <- lcmm_data %>%
  distinct(patient_id) %>%
  slice_sample(n = 100) %>%  # number sampled here 
  pull(patient_id)

spaghetti_data <- lcmm_data %>%
  filter(patient_id %in% sample_patients)

ggplot(spaghetti_data, aes(x = time, y = hospital_visits, group = patient_id)) +
  geom_line(alpha = 0.3, color = "steelblue") +
  geom_smooth(aes(group = 1), method = "loess", se = TRUE, color = "red", linewidth = 1.5) +
  labs(
    title = "Individual Trajectory Patterns, Sampled (n = 100)",
    x = "Quarters Since Treatment",
    y = "Number of Hospital Visits"
  ) +
  theme_minimal()

ggplot(lcmm_data, aes(x = factor(time), y = hospital_visits)) +
  geom_boxplot(alpha = 0.7, fill = "lightblue") +
  labs(
    title = "Visit Distribution by Time Point",
    x = "Quarters Since Treatment)",
    y = "Number of Hospital Visits"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Fitting a model with 1-class, the main question here being 
"is the 4-class model (to be fit later) superior to the 1-class model?"
```{r}
model_1class <- lcmm(
  hospital_visits ~ time + I(time^2),  # Quadratic time trend
  subject = "patient_numeric",
  ng = 1,  # number of classes 
  data = lcmm_data,
  link = "linear", # TODO: this can also change, but keep for now
  verbose = FALSE
)

# did the model converge?
model_1class$conv == 1
# Log-likelihood
model_1class$loglik
# AIC
model_1class$AIC
# BIC 
model_1class$BIC
```

By the virtue of owning the data creation process, I know that the "true" number of latent classes are 4.
The results should be superior compared to the 1-class model (as well as any other n-class), but that remains to be seen. 
** Note that a model of this complexity and size (esp bayesian?? (needs verifying)) requires input, B, that provides initial values. 
```{r}
set.seed(1008)

model_4class <- lcmm(
  hospital_visits ~ time + I(time^2),     
  mixture = ~ time + I(time^2), # different trajectories per class
  subject = "patient_numeric",
  ng = 4, # 
  data = lcmm_data,
  link = "linear",
  verbose = TRUE, 
  maxiter = 200, # increase if the model struggles to converge
  B = model_1class # **
)

# did the model converge?
model_4class$conv == 1
# Log-likelihood
model_4class$loglik
# AIC
model_4class$AIC
# BIC 
model_4class$BIC

# improvement?
round(model_1class$BIC - model_4class$BIC, 1)

summary(model_4class)
```

now we can look at class assignments to explore 
```{r}
# extracting posterior class probabilities
# it should be (10000, 6), and
# the columns read as: patient_id, most_likely_class, prob1, prob2, prob3, prob4
posterior_probs <- model_4class$pprob

# class assignments
class_assignments <- posterior_probs[, 1:2]  # patient_numeric (patient_id) and predicted class
colnames(class_assignments) <- c("patient_numeric", "predicted_class")

# class sizes
table(class_assignments$predicted_class)
round(prop.table(table(class_assignments$predicted_class)) * 100, 1)

# now that we have the posterior probabilities, it is possible to gauge the "quality" of the classification,
# first, for each individual patient, extract the predicted class with the highest posterior probability
# plainly, this means that I am only looking at my confidence level for the class that I am the most confident about
avg_posterior <- posterior_probs %>%
  as.data.frame() %>%
  rowwise() %>%
  mutate(
    max_prob = max(c_across(starts_with("prob")), na.rm = TRUE)
  ) %>%
  pull(max_prob)

round(mean(avg_posterior, na.rm = TRUE), 3)
round(median(avg_posterior, na.rm = TRUE), 3)
# how often are we over .7 confident?
round(mean(avg_posterior > 0.7, na.rm = TRUE) * 100, 1)
# how often are we over .9 confident?
round(mean(avg_posterior > 0.9, na.rm = TRUE) * 100, 1)
```

```{r}
# attach the class assignments to the original quality visits data (lcmm_data; long form)
trajectory_data <- lcmm_data %>%
  left_join(
    data.frame(
      patient_numeric = class_assignments[, "patient_numeric"],
      predicted_class = as.factor(class_assignments[, "predicted_class"])
    ),
    by = "patient_numeric"
  ) %>%
  filter(!is.na(predicted_class))

# calculate mean trajectories by class
mean_trajectories <- trajectory_data %>%
  group_by(predicted_class, time) %>%
  summarise(
    n = n(),
    mean_visits = mean(hospital_visits, na.rm = TRUE),
    se_visits = sd(hospital_visits, na.rm = TRUE) / sqrt(n()),
    lower_ci = mean_visits - 1.96 * se_visits,
    upper_ci = mean_visits + 1.96 * se_visits,
    .groups = "drop"
  )

# sample some patients to overlay individual trajectories
set.seed(1008)
sample_by_class <- trajectory_data %>%
  group_by(predicted_class) %>%
  slice_sample(n = 30) %>%  # 30 patients per class
  pull(patient_id)
indi_patient_data <- trajectory_data %>%
  filter(patient_id %in% sample_by_class)


ggplot() +
  # individual trajectories
  geom_line(
    data = indi_patient_data,
    aes(x = time, y = hospital_visits, group = patient_id, color = predicted_class),
    alpha = 0.3, linewidth = 0.3
  ) +
  # confidence bands
  geom_ribbon(
    data = mean_trajectories,
    aes(x = time, y = mean_visits, ymin = lower_ci, ymax = upper_ci, fill = predicted_class),
    alpha = 0.4
  ) +
  # mean trajectories
  geom_line(
    data = mean_trajectories,
    aes(x = time, y = mean_visits, color = predicted_class),
    linewidth = 1.5
  ) +
  scale_color_brewer(palette = "Set1", name = "Trajectory Class") +
  scale_fill_brewer(palette = "Set1", name = "Trajectory Class") +
  labs(
    title = "Hospital Visit Trajectories by Latent Class (via LCMM)",
    x = "Quarters Since Treatment)",
    y = "Number of Hospital Visits"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12)
  )

cat("\nTrajectory class characteristics:\n")
for(class in sort(unique(mean_trajectories$predicted_class))) {
  class_data <- mean_trajectories %>% filter(predicted_class == class)
  initial_visits <- class_data$mean_visits[class_data$time == 0]
  final_visits <- class_data$mean_visits[class_data$time == max(class_data$time)]
  peak_visits <- max(class_data$mean_visits)
  
  cat("Class", class, ":\n")
  cat("  - Initial visits (Q1):", round(initial_visits, 1), "\n")
  cat("  - Final visits (Q20):", round(final_visits, 1), "\n")
  cat("  - Peak visits:", round(peak_visits, 1), "\n")
  cat("  - Pattern:", ifelse(initial_visits > final_visits, "Decreasing", 
                           ifelse(initial_visits < final_visits, "Increasing", "Stable")), "\n\n")
}

```

Now, the most important. Since we have the latent classes, we now look back and attempt to characterize different classes using the baselines characteristics available to us. 
```{r}
# attach the class assignments to the original baseline characteristics data (1 row per patient)
class_characteristics <- patient_demographics %>%
  mutate(patient_numeric = as.numeric(factor(patient_id))) %>%
  left_join(
    data.frame(
      patient_numeric = class_assignments[, "patient_numeric"],
      predicted_class = class_assignments[, "predicted_class"]
    ),
    by = "patient_numeric"
  ) %>%
  filter(!is.na(predicted_class))

# Age
age_by_class <- class_characteristics %>%
  group_by(predicted_class) %>%
  summarise(
    n = n(),
    mean_age = round(mean(age, na.rm = TRUE), 1),
    sd_age = round(sd(age, na.rm = TRUE), 1),
    median_age = median(age, na.rm = TRUE),
    .groups = "drop"
  )
print(age_by_class)

# Sex
cat("sex")
sex_table <- prop.table(table(class_characteristics$predicted_class, 
                              class_characteristics$sex), 1) * 100
print(round(sex_table, 1))

# BMI
bmi_by_class <- class_characteristics %>%
  group_by(predicted_class) %>%
  summarise(
    n = n(),
    mean_bmi = round(mean(bmi, na.rm = TRUE), 1),
    sd_bmi = round(sd(bmi, na.rm = TRUE), 1),
    median_bmi = median(bmi, na.rm = TRUE),
    .groups = "drop"
  )
print(bmi_by_class)

# tb type (pulmonary vs not)
cat("tb type")
tb_type_table <- prop.table(table(class_characteristics$predicted_class, 
                                 class_characteristics$tb_type), 1) * 100
print(round(tb_type_table, 1))

# drug resistance
cat("drug resistance")
dr_table <- prop.table(table(class_characteristics$predicted_class, 
                            class_characteristics$drug_resistance), 1) * 100
print(round(dr_table, 1))

# treatment outcome
cat("outcome")
outcome_table <- prop.table(table(class_characteristics$predicted_class, 
                                 class_characteristics$treatment_outcome), 1) * 100
print(round(outcome_table, 1))

# cxr test result
cxr_by_class <- class_characteristics %>%
  group_by(predicted_class) %>%
  summarise(
    n = n(),
    mean_cxr = round(mean(baseline_cxr_severity, na.rm = TRUE), 1),
    sd_cxr = round(sd(baseline_cxr_severity, na.rm = TRUE), 1),
    median_cxr = median(baseline_cxr_severity, na.rm = TRUE),
    .groups = "drop"
  )
print(cxr_by_class)

# sputum smear
cat("sputum")
sputum_table <- prop.table(table(class_characteristics$predicted_class, 
                                 class_characteristics$baseline_sputum_smear), 1) * 100
print(round(sputum_table, 1))

# education level
cat("education level")
edu_table <- prop.table(table(class_characteristics$predicted_class, 
                              class_characteristics$education_level), 1) * 100
print(round(edu_table, 1))

# employment status
cat("employment status")
emp_table <- prop.table(table(class_characteristics$predicted_class, 
                              class_characteristics$employment_status), 1) * 100
print(round(emp_table, 1))
```

Again, having physically created the data that we are using, we can actually gauge the performance of the model.

Important thing (I say this because I forgot and was scratching my head for a bit) is that there isn't really a rhyme or reason as to why the GBTM model assigns the groups. So we have to remember that, in the creation:
Group 1: High initial visits tapering to low (typical recovery)
Group 2: Consistently low visits (good adherence, no complications)
Group 3: Moderate visits with periodic spikes (intermittent complications)
Group 4: Persistently high visits (poor outcomes/complications)

and, here, in the model output section
Group 1: Consistently high
Group 2: High initially, trending lower
Group 3: Consistently low visits

meaning the output should actually be more like:
Group 1: Consistently high -> 4
Group 2: High initially, trending lower -> 1
Group 3: Consistently low  -> 2

Note that, as expected, the model struggles to pick up on Group 3
```{r}
validation_data <- class_characteristics %>%
  dplyr::select(patient_id, trajectory_group, predicted_class) %>%
  mutate(
    true_class = ifelse(predicted_class == 1, 4,
                 ifelse(predicted_class == 2, 1,
                 ifelse(predicted_class == 3, 2, 
                                              3)))
  )

confusion_matrix <- table(
  True = validation_data$trajectory_group,
  Predicted = validation_data$true_class
)
print(confusion_matrix)

# accuracy - overall
overall_accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
round(overall_accuracy * 100, 1)
# class-specific
accuracy_summary <- c(4033, 2502, 0, 466) # referring to the true correct values in the confusion_matrix
class_accuracy <- accuracy_summary / rowSums(confusion_matrix)
for(i in 1:length(class_accuracy)) {
  cat("  True class", i, ":", round(class_accuracy[i] * 100, 1), "%\n")
}

# Adjusted Rand Index (if mclust available)
if(require(mclust, quietly = TRUE)) {
  ari <- adjustedRandIndex(validation_data$trajectory_group, validation_data$true_class)
  cat("Adjusted Rand Index:", round(ari, 3)) # where 1 is perfect agreement & 0 is random agreement 
}
```

Now the question is, if we DO NOT know the TRUE latent class size, can we still be able to identify it via the model selection criteria (i.e. lowest BIC)? 

Trying it with latent classes 2 through 6
```{r}
fit_lcmm_models <- function(data, k_range = 2:6) {
  
  models <- list()
  model_fit <- tibble()
  
  for(k in k_range) {
    cat("Fitting LCMM", k, "group model...\n")
    
    tryCatch({
      # Fit polynomial mixed model for trajectories
      fit <- lcmm(
        hospital_visits ~ poly(time, 2),  # Quadratic time trend
        mixture = ~ poly(time, 2),        # Different trajectories per group
        subject = "patient_numeric",
        ng = k,
        data = data,
        link = "linear",
        verbose = FALSE,
        B = model_1class
      )
      
      models[[paste0("k", k)]] <- fit
      
      # Store fit statistics
      model_fit <- bind_rows(model_fit, tibble(
        k = k,
        loglik = fit$loglik,
        aic = fit$AIC,
        bic = fit$BIC,
        converged = fit$conv
      ))
      
    }, error = function(e) {
      cat("Error fitting", k, "group model:", e$message, "\n")
    })
  }
  
  return(list(models = models, fit_stats = model_fit))
}

# k = 6, takes far too long (for 10,000 patients... )
# the actual data has 400,000 patients :/ 
lcmm_results <- fit_lcmm_models(visits_long, k_range = 2:5)

print(lcmm_results$fit_stats)

# select best model based on BIC
if(nrow(lcmm_results$fit_stats) > 0) {
  best_k_lcmm <- lcmm_results$fit_stats$k[which.min(lcmm_results$fit_stats$bic)]
  cat("\nBest model based on BIC:", best_k_lcmm, "groups\n")
  
  best_lcmm <- lcmm_results$models[[paste0("k", best_k_lcmm)]]
  
  lcmm_groups <- best_lcmm$pprob[,1:2]  # patient_id & predicted class
  colnames(lcmm_groups) <- c("patient_numeric", "predicted_group")
  
  cat("Group sizes (LCMM):\n")
  print(table(lcmm_groups[,2]))
}

```

